{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from brainglobe_atlasapi import BrainGlobeAtlas\n",
    "import iss_preprocess as iss\n",
    "import scanpy as sc\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42  # Use Type 3 fonts (TrueType) for selectable text\n",
    "matplotlib.rcParams['ps.fonttype'] = 42  # For EPS, if relevant\n",
    "\n",
    "bg_atlas = BrainGlobeAtlas(\"allen_mouse_10um\", check_latest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by_presynaptics = False\n",
    "filter_shared_bc_starters = True\n",
    "filter_by_annotation = False\n",
    "use_centroid_cluster_mapping = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"becalia_rabies_barseq/BRAC8498.3e/chamber_07\"\n",
    "processed_path = iss.io.get_processed_path(data_path)\n",
    "\n",
    "#Load data from mCherry curated cells and barcodes with ed2, minimum match ?, ed correction weighting only first 10 bases\n",
    "ara_starters = pd.read_pickle(processed_path.parent / \"analysis\" / \"merged_cell_df_curated_mcherry.pkl\")\n",
    "ara_starters = ara_starters[ara_starters[\"main_barcode\"].notna()]\n",
    "print(\"Before filtering:\")\n",
    "print(f\"Number of barcoded cells: {ara_starters.shape[0]}\")\n",
    "print(f\"Number of barcodes: {ara_starters['main_barcode'].nunique()}\")\n",
    "print(f\"Number of presynaptic cells: {ara_starters[ara_starters['is_starter'] == False].shape[0]}\")\n",
    "print(f\"Number of starter cells: {ara_starters[ara_starters['is_starter'] == True].shape[0]}\\n\")\n",
    "\n",
    "# Step 1: Remove starters with shared barcodes\n",
    "ara_is_starters = pd.read_pickle(processed_path.parent / \"analysis\" / \"merged_cell_df_curated_mcherry.pkl\")\n",
    "ara_is_starters = ara_is_starters[ara_is_starters[\"main_barcode\"].notna()]\n",
    "\n",
    "# Assuming ara_is_starters is your dataframe\n",
    "def shorten_barcodes(barcodes):\n",
    "    return [barcode[:10] for barcode in barcodes]\n",
    "\n",
    "ara_is_starters['all_barcodes'] = ara_is_starters['all_barcodes'].apply(shorten_barcodes)\n",
    "ara_is_starters['main_barcode'] = ara_is_starters['main_barcode'].apply(lambda x: x[:10])\n",
    "\n",
    "if filter_shared_bc_starters:\n",
    "    # Flatten all barcodes from starter cells to count their occurrences\n",
    "    starter_barcodes_counts = (\n",
    "        ara_is_starters[ara_is_starters[\"is_starter\"] == True][\"all_barcodes\"]\n",
    "        .explode()\n",
    "        .value_counts()\n",
    "    )\n",
    "\n",
    "    # Identify barcodes that are unique to a single starter cell\n",
    "    unique_starter_barcodes = starter_barcodes_counts[starter_barcodes_counts == 1].index\n",
    "\n",
    "    # Filter starter cells where all their barcodes are unique among starter cells\n",
    "    starter_cells_with_unique_barcodes = ara_is_starters[\n",
    "        (ara_is_starters[\"is_starter\"] == True)\n",
    "        & (ara_is_starters[\"all_barcodes\"].apply(lambda barcodes: all(b in unique_starter_barcodes for b in barcodes)))\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # Filter presynaptic cells that contain at least one of these barcodes\n",
    "    presynaptic_cells_with_shared_barcodes = ara_is_starters[\n",
    "        (ara_is_starters[\"is_starter\"] == False)\n",
    "        & (ara_is_starters[\"all_barcodes\"].apply(lambda barcodes: any(b in unique_starter_barcodes for b in barcodes)))\n",
    "    ]\n",
    "    # Combine the filtered starter cells and the filtered presynaptic cells\n",
    "    ara_starters = pd.concat([starter_cells_with_unique_barcodes, presynaptic_cells_with_shared_barcodes])\n",
    "ara_starters = ara_starters.rename(columns={\"is_starter\": \"starter\"})\n",
    "\n",
    "\n",
    "print(\"After shared starter cell count filtering:\")\n",
    "print(f\"Number of barcoded cells: {ara_starters.shape[0]}\")\n",
    "print(f\"Number of barcodes: {ara_starters['main_barcode'].nunique()}\")\n",
    "print(f\"Number of presynaptic cells: {ara_starters[ara_starters['starter'] == False].shape[0]}\")\n",
    "print(f\"Number of starter cells: {ara_starters[ara_starters['starter'] == True].shape[0]}\\n\")\n",
    "\n",
    "\n",
    "# Step 2: Identify barcodes found in more than 4 non-starter cells\n",
    "if filter_by_presynaptics:\n",
    "    if False:\n",
    "        non_starter_barcodes_counts = (\n",
    "            ara_starters[ara_starters[\"starter\"] == False][\"all_barcodes\"]\n",
    "            .explode()\n",
    "            .value_counts()\n",
    "        )\n",
    "        barcodes_to_keep = non_starter_barcodes_counts[non_starter_barcodes_counts > 4].index.values\n",
    "        ara_starters = ara_starters[ara_starters[\"all_barcodes\"].apply(lambda x: any([b in barcodes_to_keep for b in x]))]\n",
    "\n",
    "\n",
    "    # Step 1: Identify barcodes found in more than 4 non-starter cells\n",
    "    non_starter_barcodes_counts = (\n",
    "        ara_starters[ara_starters[\"starter\"] == False][\"all_barcodes\"]\n",
    "        .explode()\n",
    "        .value_counts()\n",
    "    )\n",
    "    barcodes_to_keep = non_starter_barcodes_counts[non_starter_barcodes_counts > 4].index.values\n",
    "\n",
    "    # Step 2: Remove barcodes that aren't present in 5 or more non-starter cells from all_barcodes\n",
    "    def filter_barcodes_and_update_counts(row):\n",
    "        filtered_barcodes = []\n",
    "        filtered_counts = []\n",
    "        total_removed = 0\n",
    "        \n",
    "        for barcode, count in zip(row[\"all_barcodes\"], row[\"n_spots_per_barcode\"]):\n",
    "            if barcode in barcodes_to_keep:\n",
    "                filtered_barcodes.append(barcode)\n",
    "                filtered_counts.append(count)\n",
    "            else:\n",
    "                total_removed += count\n",
    "        \n",
    "        row[\"all_barcodes\"] = filtered_barcodes\n",
    "        row[\"n_spots_per_barcode\"] = filtered_counts\n",
    "        row[\"total_n_spots\"] -= total_removed\n",
    "        \n",
    "        return row\n",
    "\n",
    "    ara_starters = ara_starters.apply(filter_barcodes_and_update_counts, axis=1)\n",
    "    ara_starters = ara_starters[ara_starters[\"total_n_spots\"] > 0]\n",
    "    print(\"After presynaptic cell count filtering:\")\n",
    "    print(f\"Number of barcoded cells: {ara_starters.shape[0]}\")\n",
    "    print(f\"Number of barcodes: {ara_starters['main_barcode'].nunique()}\")\n",
    "    print(f\"Number of presynaptic cells: {ara_starters[ara_starters['starter'] == False].shape[0]}\")\n",
    "    print(f\"Number of starter cells: {ara_starters[ara_starters['starter'] == True].shape[0]}\\n\")\n",
    "\n",
    "\n",
    "# Step 2: Add cell type annotations and cell correlations to cluster centroids\n",
    "adata = sc.read_h5ad(processed_path.parent / \"analysis\" / \"adata_annotated.h5ad\")\n",
    "ara_starters[\"Annotated_clusters\"] = adata.obs.Annotated_clusters\n",
    "ara_starters[\"gene_total_counts\"] = adata.obs.total_counts\n",
    "ara_starters[\"n_genes\"] = adata.obs.n_genes_by_counts\n",
    "correlation_cells = pd.read_csv(\"/nemo/project/proj-znamenp-barseq/processed/becalia_rabies_barseq/BRAC8498.3e/analysis/correlation_scores_all_barcoded.csv\", index_col=0)\n",
    "ara_starters = ara_starters.join(correlation_cells)\n",
    "\n",
    "if use_centroid_cluster_mapping:\n",
    "    # Step 3: See how many cells would be left if we immediately filter to cells that already have an annotated cluster\n",
    "    ara_over25_genes = ara_starters.dropna(subset=[\"Annotated_clusters\"])\n",
    "    print(\"After annotated cluster filtering:\")\n",
    "    print(f\"Number of barcoded cells: {ara_over25_genes.shape[0]}\")\n",
    "    print(f\"Number of barcodes: {ara_over25_genes['main_barcode'].nunique()}\")\n",
    "    print(f\"Number of presynaptic cells: {ara_over25_genes[ara_over25_genes['starter'] == False].shape[0]}\")\n",
    "    print(f\"Number of starter cells: {ara_over25_genes[ara_over25_genes['starter'] == True].shape[0]}\\n\")\n",
    "\n",
    "\n",
    "    # Filter to just cells with a high correlation to a cell type cluster centroid\n",
    "    ara_starters.dropna(subset=[\"best_cluster\"], inplace=True)\n",
    "    ara_starters = ara_starters[ara_starters.best_cluster != \"Zero_correlation\"]\n",
    "    ara_starters = ara_starters[ara_starters.best_score > 0.2]\n",
    "    ara_starters['Clusters'] = ara_starters['Annotated_clusters'].fillna(ara_starters['best_cluster'])\n",
    "    print(\"After cell type cluster centroid filtering:\")\n",
    "    print(f\"Number of barcoded cells: {ara_starters.shape[0]}\")\n",
    "    print(f\"Number of barcodes: {ara_starters['main_barcode'].nunique()}\")\n",
    "    print(f\"Number of presynaptic cells: {ara_starters[ara_starters['starter'] == False].shape[0]}\")\n",
    "    print(f\"Number of starter cells: {ara_starters[ara_starters['starter'] == True].shape[0]}\\n\")\n",
    "\n",
    "else:\n",
    "    if filter_by_annotation:\n",
    "        ara_starters = ara_starters.dropna(subset=[\"Annotated_clusters\"])\n",
    "        print(\"After annotated cluster filtering:\")\n",
    "        print(f\"Number of barcoded cells: {ara_starters.shape[0]}\")\n",
    "        print(f\"Number of barcodes: {ara_starters['main_barcode'].nunique()}\")\n",
    "        print(f\"Number of presynaptic cells: {ara_starters[ara_starters['starter'] == False].shape[0]}\")\n",
    "        print(f\"Number of starter cells: {ara_starters[ara_starters['starter'] == True].shape[0]}\\n\")\n",
    "\n",
    "def get_ancestor_rank1(area_acronym):\n",
    "    try:\n",
    "        ancestors = bg_atlas.get_structure_ancestors(area_acronym)\n",
    "        if \"TH\" in ancestors:\n",
    "            return \"TH\"\n",
    "        elif \"RSP\" in ancestors:\n",
    "            return \"RSP\"\n",
    "        elif \"TEa\" in ancestors:\n",
    "            return \"TEa\"\n",
    "        elif \"AUD\" in ancestors:\n",
    "            return \"AUD\"\n",
    "        elif \"VISp\" in ancestors:\n",
    "            return area_acronym\n",
    "        elif \"VIS\" in ancestors:\n",
    "            return ancestors[-1]\n",
    "        else:\n",
    "            return ancestors[1] if len(ancestors) > 1 else 'Unknown'\n",
    "    except KeyError:\n",
    "        return 'Unknown'\n",
    "\n",
    "ara_starters[\"area_acronym_ancestor_rank1\"] = ara_starters[\"area_acronym\"].apply(get_ancestor_rank1)\n",
    "\n",
    "\n",
    "cortical_areas = {\n",
    "    #'outside': \"outside\",\n",
    "    #'root': \"outside\",\n",
    "       \n",
    "    ### Auditory primary\n",
    "    'AUDp1': \"AUDp\",\n",
    "    'AUDp2/3': \"AUDp\",\n",
    "    'AUDp4': \"AUDp\",\n",
    "    'AUDp5': \"AUDp\",\n",
    "    'AUDp6a': \"AUDp\",\n",
    "    'AUDp6b': \"AUDp\",\n",
    "    ### Auditory posterior\n",
    "    'AUDpo1': \"AUDpo\",\n",
    "    'AUDpo2/3': \"AUDpo\",\n",
    "    'AUDpo4': \"AUDpo\",\n",
    "    'AUDpo5': \"AUDpo\",\n",
    "    'AUDpo6a': \"AUDpo\",\n",
    "    'AUDpo6b': \"AUDpo\",\n",
    "    ### Auditory ventral\n",
    "    'AUDv1': \"AUDv\",\n",
    "    'AUDv2/3': \"AUDv\",\n",
    "    'AUDv4': \"AUDv\",\n",
    "    'AUDv5': \"AUDv\",\n",
    "    'AUDv6a': \"AUDv\",\n",
    "    'AUDv6b': \"AUDv\",\n",
    "    ### Retrosplenial lateral agranular\n",
    "    'RSPagl1': \"RSP\",\n",
    "    'RSPagl2/3': \"RSP\",\n",
    "    'RSPagl5': \"RSP\",\n",
    "    'RSPagl6a': \"RSP\",\n",
    "    'RSPagl6b': \"RSP\",\n",
    "    ### Retrosplenial dorsal\n",
    "    'RSPd1': \"RSP\",\n",
    "    'RSPd2/3': \"RSP\",\n",
    "    'RSPd5': \"RSP\",\n",
    "    'RSPd6a': \"RSP\",\n",
    "    'RSPd6b': \"RSP\",\n",
    "    ### Retrosplenial ventral\n",
    "    'RSPv1': \"RSP\",\n",
    "    'RSPv2/3': \"RSP\",\n",
    "    'RSPv5': \"RSP\",\n",
    "    'RSPv6a': \"RSP\",\n",
    "    ### Visual antero-lateral\n",
    "    'VISal1': \"VISal\",\n",
    "    'VISal2/3': \"VISal\",\n",
    "    'VISal4': \"VISal\",\n",
    "    'VISal5': \"VISal\",\n",
    "    'VISal6a': \"VISal\",\n",
    "    'VISal6b': \"VISal\",\n",
    "    ### Visual lateral\n",
    "    'VISl1': \"VISl\",\n",
    "    'VISl2/3': \"VISl\",\n",
    "    'VISl4': \"VISl\",\n",
    "    'VISl5': \"VISl\",\n",
    "    'VISl6a': \"VISl\",\n",
    "    'VISl6b': \"VISl\",\n",
    "    ### Visual laterointermediate\n",
    "    'VISli1': \"VISli\",\n",
    "    'VISli2/3': \"VISli\",\n",
    "    'VISli4': \"VISli\",\n",
    "    'VISli5': \"VISli\",\n",
    "    'VISli6a': \"VISli\",\n",
    "    'VISli6b': \"VISli\",\n",
    "    ### Visual primary\n",
    "    'VISp1': \"VISp\",\n",
    "    'VISp2/3': \"VISp\",\n",
    "    'VISp4': \"VISp\",\n",
    "    'VISp5': \"VISp\",\n",
    "    'VISp6a': \"VISp\",\n",
    "    'VISp6b': \"VISp\",\n",
    "    ### Visual posteromedial\n",
    "    'VISpm1': \"VISpm\",\n",
    "    'VISpm2/3': \"VISpm\",\n",
    "    'VISpm4': \"VISpm\",\n",
    "    'VISpm5': \"VISpm\",\n",
    "    'VISpm6a': \"VISpm\",\n",
    "    'VISpm6b': \"VISpm\",\n",
    "    ### TEa\n",
    "    'TEa1': \"TEa\",\n",
    "    'TEa2/3': \"TEa\",\n",
    "    'TEa4': \"TEa\",\n",
    "    'TEa5': \"TEa\",\n",
    "    'TEa6a': \"TEa\",\n",
    "    'TEa6b': \"TEa\",\n",
    "\n",
    "    ### Hippocampal Areas\n",
    "    'HPF': \"hippocampal\",\n",
    "    'SUB': \"hippocampal\",\n",
    "    'POST': \"hippocampal\",\n",
    "    'ProS': \"hippocampal\",\n",
    "    'CA1': \"hippocampal\",\n",
    "    'CA2': \"hippocampal\",\n",
    "    'CA3': \"hippocampal\",\n",
    "    'DG-mo': \"hippocampal\",\n",
    "    'DG-sg': \"hippocampal\",\n",
    "    'DG-po': \"hippocampal\",\n",
    "    \n",
    "    ### Thalamus\n",
    "    'IGL': \"TH\",\n",
    "    \"LGd-ip\" : \"TH\",\n",
    "    \"LGd-sh\" : \"TH\",\n",
    "    'LGd-co': \"TH\",\n",
    "    'MG': \"TH\",\n",
    "    'MGd': \"TH\",\n",
    "    'MGv': \"TH\",\n",
    "    \"LAT\" : \"TH\",\n",
    "    'IntG': \"TH\",\n",
    "    'LGd': \"TH\",\n",
    "    'LGv': \"TH\",\n",
    "    \"VENT\" : \"TH\",\n",
    "    \"PP\" : \"TH\",\n",
    "    \"PIL\" : \"TH\",\n",
    "    \"VPM\" : \"TH\",\n",
    "    \"VPMpc\" : \"TH\",\n",
    "    \"VM\" : \"TH\",\n",
    "    \"POL\" : \"TH\",\n",
    "    'SGN': \"TH\",\n",
    "    'LP': \"TH\",\n",
    "    'PoT': \"TH\",\n",
    "    'Eth': \"TH\",\n",
    "    'TH': \"TH\",\n",
    "    'MGm': \"TH\",\n",
    "    \n",
    "    ### Fiber Tracts\n",
    "    'fiber tracts': \"fiber_tract\",\n",
    "    'mlf': \"fiber_tract\",\n",
    "    'optic': \"fiber_tract\",\n",
    "    'ar': \"fiber_tract\",\n",
    "    'fr': \"fiber_tract\",\n",
    "    'ml': \"fiber_tract\",\n",
    "    'rust': \"fiber_tract\",\n",
    "    'cpd': \"fiber_tract\",\n",
    "    'cc': \"fiber_tract\",\n",
    "    'lfbst': \"fiber_tract\",\n",
    "    'cing': \"fiber_tract\",\n",
    "    'hc': \"fiber_tract\",\n",
    "    'scwm': \"fiber_tract\",\n",
    "    'fp': \"fiber_tract\",\n",
    "    'dhc': \"fiber_tract\",\n",
    "    'alv': \"fiber_tract\",\n",
    "    'or': \"fiber_tract\",\n",
    "    'bsc': \"fiber_tract\",\n",
    "    'pc': \"fiber_tract\",\n",
    "    'ec': \"fiber_tract\",\n",
    "    'opt': \"fiber_tract\",\n",
    "    'vtd': \"fiber_tract\",\n",
    "    'mtg': \"fiber_tract\",\n",
    "    'EW': \"fiber_tract\",\n",
    "    'bic': \"fiber_tract\",\n",
    "    'amc': \"fiber_tract\",\n",
    "    'act': \"fiber_tract\",\n",
    "    'st': \"fiber_tract\",\n",
    "    'apd': \"fiber_tract\",\n",
    "    'lab': \"fiber_tract\",\n",
    "    'df': \"fiber_tract\",\n",
    "    'fi': \"fiber_tract\",\n",
    "    'fxpo': \"fiber_tract\",\n",
    "    'mct': \"fiber_tract\",\n",
    "    'fx': \"fiber_tract\",\n",
    "    'vhc': \"fiber_tract\",\n",
    "    'stc': \"fiber_tract\",\n",
    "    'mfbc': \"fiber_tract\",\n",
    "\n",
    "    ### Non-cortical areas\n",
    "    'SCzo': \"non_cortical\",\n",
    "    'SCop': \"non_cortical\",\n",
    "    'MB': \"non_cortical\",\n",
    "    'NPC': \"non_cortical\",\n",
    "    'MPT': \"non_cortical\",\n",
    "    'PPT': \"non_cortical\",\n",
    "    'NOT': \"non_cortical\",\n",
    "    'OP': \"non_cortical\",\n",
    "    'APN': \"non_cortical\",\n",
    "    'PAG': \"non_cortical\",\n",
    "    'SCO': \"non_cortical\",\n",
    "    'RPF': \"non_cortical\",\n",
    "    'AQ': \"non_cortical\",\n",
    "    'MRN': \"non_cortical\",\n",
    "    'FF': \"non_cortical\",\n",
    "    'HY': \"non_cortical\",\n",
    "    'ZI': \"non_cortical\",\n",
    "    'SCig': \"non_cortical\",\n",
    "    'SCsg': \"non_cortical\",\n",
    "    'EPd': \"non_cortical\",\n",
    "    'SCiw': \"non_cortical\",\n",
    "    'ND': \"non_cortical\",\n",
    "    'INC': \"non_cortical\",\n",
    "    'LT': \"non_cortical\",\n",
    "    'SNr': \"non_cortical\",\n",
    "    'SNc': \"non_cortical\",\n",
    "    'VTA': \"non_cortical\",\n",
    "    'SCdg': \"non_cortical\",\n",
    "    'SCdw': \"non_cortical\",\n",
    "    'csc': \"non_cortical\",\n",
    "    'RN': \"non_cortical\",\n",
    "    'MA3': \"non_cortical\",\n",
    "    'DT': \"non_cortical\",\n",
    "    'MT': \"non_cortical\",\n",
    "    'ENTl3': \"non_cortical\",\n",
    "    'ENTl2': \"non_cortical\",\n",
    "    'ENTl1': \"non_cortical\",\n",
    "    'SPFp': \"non_cortical\",\n",
    "}\n",
    "\n",
    "cortical_layers = {\n",
    "    #'outside': \"outside\",\n",
    "    #'root': \"outside\",\n",
    "\n",
    "    ### Layer 1\n",
    "    'RSPd1': \"L1\",\n",
    "    'RSPv1': \"L1\",\n",
    "    'RSPagl1': \"L1\",\n",
    "    'VISpm1': \"L1\",\n",
    "    'VISp1': \"L1\",\n",
    "    'VISal1': \"L1\",\n",
    "    'VISl1': \"L1\",\n",
    "    'VISli1': \"L1\",\n",
    "    'TEa1': \"L1\",\n",
    "    'AUDpo1': \"L1\",\n",
    "    'AUDp1': \"L1\",\n",
    "    'AUDv1': \"L1\",\n",
    "    'ECT1': \"L1\",\n",
    "    'PERI1': \"L1\",\n",
    "\n",
    "    ### Layer 2/3\n",
    "    'RSPd2/3': \"L2/3\",\n",
    "    'RSPv2/3': \"L2/3\",\n",
    "    'RSPagl2/3': \"L2/3\",\n",
    "    'VISpm2/3': \"L2/3\",\n",
    "    'VISp2/3': \"L2/3\",\n",
    "    'VISal2/3': \"L2/3\",\n",
    "    'VISl2/3': \"L2/3\",\n",
    "    'VISli2/3': \"L2/3\",\n",
    "    'TEa2/3': \"L2/3\",\n",
    "    'AUDpo2/3': \"L2/3\",\n",
    "    'AUDp2/3': \"L2/3\",\n",
    "    'AUDv2/3': \"L2/3\",\n",
    "    'ECT2/3': \"L2/3\",\n",
    "    'PERI2/3': \"L2/3\",\n",
    "\n",
    "    ### Layer 4\n",
    "    'VISpm4': \"L4\",\n",
    "    'VISp4': \"L4\",\n",
    "    'VISal4': \"L4\",\n",
    "    'VISl4': \"L4\",\n",
    "    'VISli4': \"L4\",\n",
    "    'TEa4': \"L4\",\n",
    "    'AUDpo4': \"L4\",\n",
    "    'AUDp4': \"L4\",\n",
    "    'AUDv4': \"L4\",\n",
    "    'ECT4': \"L4\",\n",
    "\n",
    "    ### Layer 5\n",
    "    'RSPd5': \"L5\",\n",
    "    'RSPv5': \"L5\",\n",
    "    'RSPagl5': \"L5\",\n",
    "    'VISpm5': \"L5\",\n",
    "    'VISp5': \"L5\",\n",
    "    'VISal5': \"L5\",\n",
    "    'VISl5': \"L5\",\n",
    "    'VISli5': \"L5\",\n",
    "    'TEa5': \"L5\",\n",
    "    'AUDpo5': \"L5\",\n",
    "    'AUDp5': \"L5\",\n",
    "    'AUDv5': \"L5\",\n",
    "    'ECT5': \"L5\",\n",
    "    'PERI5': \"L5\",\n",
    "    'ENTl5': \"L5\",\n",
    "\n",
    "    ### Layer 6a\n",
    "    'RSPv6a': \"L6a\",\n",
    "    'RSPd6a': \"L6a\",\n",
    "    'RSPagl6a': \"L6a\",\n",
    "    'VISpm6a': \"L6a\",\n",
    "    'VISp6a': \"L6a\",\n",
    "    'VISal6a': \"L6a\",\n",
    "    'VISl6a': \"L6a\",\n",
    "    'VISli6a': \"L6a\",\n",
    "    'TEa6a': \"L6a\",\n",
    "    'AUDpo6a': \"L6a\",\n",
    "    'AUDp6a': \"L6a\",\n",
    "    'AUDv6a': \"L6a\",\n",
    "    'ECT6a': \"L6a\",\n",
    "    'ENTl6a': \"L6a\",\n",
    "    'PERI6a': \"L6a\",\n",
    "\n",
    "    ### Layer 6b\n",
    "    'RSPv6b': \"L6b\",\n",
    "    'RSPd6b': \"L6b\",\n",
    "    'RSPagl6b': \"L6b\",\n",
    "    'VISpm6b': \"L6b\",\n",
    "    'VISp6b': \"L6b\",\n",
    "    'VISal6b': \"L6b\",\n",
    "    'VISl6b': \"L6b\",\n",
    "    'VISli6b': \"L6b\",\n",
    "    'TEa6b': \"L6b\",\n",
    "    'AUDpo6b': \"L6b\",\n",
    "    'AUDp6b': \"L6b\",\n",
    "    'AUDv6b': \"L6b\",\n",
    "    'ECT6b': \"L6b\",\n",
    "    'PERI6b': \"L6b\",\n",
    "    'VISpor6b': \"L6b\",\n",
    "\n",
    "    ### Hippocampal Areas\n",
    "    'HPF': \"hippocampal\",\n",
    "    'SUB': \"hippocampal\",\n",
    "    'POST': \"hippocampal\",\n",
    "    'ProS': \"hippocampal\",\n",
    "    'CA1': \"hippocampal\",\n",
    "    'CA2': \"hippocampal\",\n",
    "    'CA3': \"hippocampal\",\n",
    "    'DG-mo': \"hippocampal\",\n",
    "    'DG-sg': \"hippocampal\",\n",
    "    'DG-po': \"hippocampal\",\n",
    "    ### Thalamus\n",
    "    'IGL': \"TH\",\n",
    "    \"LGd-ip\" : \"TH\",\n",
    "    \"LGd-sh\" : \"TH\",\n",
    "    'LGd-co': \"TH\",\n",
    "    'MG': \"TH\",\n",
    "    'MGd': \"TH\",\n",
    "    'MGv': \"TH\",\n",
    "    \"LAT\" : \"TH\",\n",
    "    'IntG': \"TH\",\n",
    "    'LGd': \"TH\",\n",
    "    'LGv': \"TH\",\n",
    "    \"VENT\" : \"TH\",\n",
    "    \"PP\" : \"TH\",\n",
    "    \"PIL\" : \"TH\",\n",
    "    \"VPM\" : \"TH\",\n",
    "    \"VPMpc\" : \"TH\",\n",
    "    \"VM\" : \"TH\",\n",
    "    \"POL\" : \"TH\",\n",
    "    'SGN': \"TH\",\n",
    "    'LP': \"TH\",\n",
    "    'PoT': \"TH\",\n",
    "    'Eth': \"TH\",\n",
    "    'TH': \"TH\",\n",
    "    'MGm': \"TH\",\n",
    "    \n",
    "    ### Fiber Tracts\n",
    "    'fiber tracts': \"fiber_tract\",\n",
    "    'mlf': \"fiber_tract\",\n",
    "    'optic': \"fiber_tract\",\n",
    "    'ar': \"fiber_tract\",\n",
    "    'fr': \"fiber_tract\",\n",
    "    'ml': \"fiber_tract\",\n",
    "    'rust': \"fiber_tract\",\n",
    "    'cpd': \"fiber_tract\",\n",
    "    'cc': \"fiber_tract\",\n",
    "    'lfbst': \"fiber_tract\",\n",
    "    'cing': \"fiber_tract\",\n",
    "    'hc': \"fiber_tract\",\n",
    "    'scwm': \"fiber_tract\",\n",
    "    'fp': \"fiber_tract\",\n",
    "    'dhc': \"fiber_tract\",\n",
    "    'alv': \"fiber_tract\",\n",
    "    'or': \"fiber_tract\",\n",
    "    'bsc': \"fiber_tract\",\n",
    "    'pc': \"fiber_tract\",\n",
    "    'ec': \"fiber_tract\",\n",
    "    'opt': \"fiber_tract\",\n",
    "    'vtd': \"fiber_tract\",\n",
    "    'mtg': \"fiber_tract\",\n",
    "    'EW': \"fiber_tract\",\n",
    "    'bic': \"fiber_tract\",\n",
    "    'amc': \"fiber_tract\",\n",
    "    'act': \"fiber_tract\",\n",
    "    'st': \"fiber_tract\",\n",
    "    'apd': \"fiber_tract\",\n",
    "    'lab': \"fiber_tract\",\n",
    "    'df': \"fiber_tract\",\n",
    "    'fi': \"fiber_tract\",\n",
    "    'fxpo': \"fiber_tract\",\n",
    "    'mct': \"fiber_tract\",\n",
    "    'fx': \"fiber_tract\",\n",
    "    'vhc': \"fiber_tract\",\n",
    "    'stc': \"fiber_tract\",\n",
    "    'mfbc': \"fiber_tract\",\n",
    "\n",
    "    ### Non-cortical areas\n",
    "    'SCzo': \"non_cortical\",\n",
    "    'SCop': \"non_cortical\",\n",
    "    'MB': \"non_cortical\",\n",
    "    'NPC': \"non_cortical\",\n",
    "    'MPT': \"non_cortical\",\n",
    "    'PPT': \"non_cortical\",\n",
    "    'NOT': \"non_cortical\",\n",
    "    'OP': \"non_cortical\",\n",
    "    'APN': \"non_cortical\",\n",
    "    'PAG': \"non_cortical\",\n",
    "    'SCO': \"non_cortical\",\n",
    "    'RPF': \"non_cortical\",\n",
    "    'AQ': \"non_cortical\",\n",
    "    'MRN': \"non_cortical\",\n",
    "    'FF': \"non_cortical\",\n",
    "    'HY': \"non_cortical\",\n",
    "    'ZI': \"non_cortical\",\n",
    "    'SCig': \"non_cortical\",\n",
    "    'SCsg': \"non_cortical\",\n",
    "    'EPd': \"non_cortical\",\n",
    "    'SCiw': \"non_cortical\",\n",
    "    'ND': \"non_cortical\",\n",
    "    'INC': \"non_cortical\",\n",
    "    'LT': \"non_cortical\",\n",
    "    'SNr': \"non_cortical\",\n",
    "    'SNc': \"non_cortical\",\n",
    "    'VTA': \"non_cortical\",\n",
    "    'SCdg': \"non_cortical\",\n",
    "    'SCdw': \"non_cortical\",\n",
    "    'csc': \"non_cortical\",\n",
    "    'RN': \"non_cortical\",\n",
    "    'MA3': \"non_cortical\",\n",
    "    'DT': \"non_cortical\",\n",
    "    'MT': \"non_cortical\",\n",
    "    'ENTl3': \"non_cortical\",\n",
    "    'ENTl2': \"non_cortical\",\n",
    "    'ENTl1': \"non_cortical\",\n",
    "    'SPFp': \"non_cortical\",\n",
    "}\n",
    "\n",
    "ara_starters['cortical_area'] = ara_starters['area_acronym'].map(cortical_areas)\n",
    "ara_starters['cortical_layer'] = ara_starters['area_acronym'].map(cortical_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw counts starter area by presyn area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data\n",
    "starters = ara_starters[ara_starters['starter'] == True]\n",
    "non_starters = ara_starters[ara_starters['starter'] == False]\n",
    "\n",
    "# Creating the confusion matrix\n",
    "confusion_matrix = pd.DataFrame(0, index=non_starters['area_acronym_ancestor_rank1'].unique(), columns=starters['area_acronym_ancestor_rank1'].unique())\n",
    "\n",
    "for _, starter_row in starters.iterrows():\n",
    "    main_barcode = starter_row['main_barcode']\n",
    "    starter_area = starter_row['area_acronym_ancestor_rank1']\n",
    "    \n",
    "    linked_non_starters = non_starters[non_starters['main_barcode'] == main_barcode]\n",
    "    for _, non_starter_row in linked_non_starters.iterrows():\n",
    "        non_starter_area = non_starter_row['area_acronym_ancestor_rank1']\n",
    "        confusion_matrix.loc[non_starter_area, starter_area] += 1\n",
    "\n",
    "# Sort the confusion matrix by index and columns alphabetically\n",
    "confusion_matrix = confusion_matrix.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "# Remove rows and columns with all zeros\n",
    "filtered_confusion_matrix = confusion_matrix.loc[(confusion_matrix != 0).any(axis=1)]\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.loc[:, (filtered_confusion_matrix != 0).any(axis=0)]\n",
    "\n",
    "# Define the labels to drop\n",
    "labels_to_drop = [\"Unknown\", \"fiber tracts\", \"grey\", \"ECT\"]\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.drop(labels=labels_to_drop, axis=0, errors='ignore')\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.drop(labels=[\"Unknown\", \"fiber tracts\"], axis=1, errors='ignore')\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.drop(labels=['VISli','VISal', 'AUD','RSP','TEa','TH',], axis=1, errors='ignore')\n",
    "\n",
    "\n",
    "# Filter to include only areas of interest\n",
    "areas_of_interest = [\n",
    "    'VISp1',\n",
    "    'VISp2/3',\n",
    "    'VISp4',\n",
    "    'VISp5',\n",
    "    'VISp6a',\n",
    "    'VISp6b',\n",
    "    'VISal',\n",
    "    'VISl',\n",
    "    'VISli',\n",
    "    'VISpm',\n",
    "    'RSP',\n",
    "    'AUD',\n",
    "    'TEa',\n",
    "    'TH',\n",
    "]\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.reindex(index=areas_of_interest, columns=areas_of_interest, fill_value=0)\n",
    "\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.loc[areas_of_interest, areas_of_interest]\n",
    "filtered_confusion_matrix = filtered_confusion_matrix.drop(labels=['VISli', 'VISal', 'AUD','RSP','TEa','TH',], axis=1, errors='ignore')\n",
    "#filtered_confusion_matrix = filtered_confusion_matrix.drop(labels=['TH',], axis=0, errors='ignore')\n",
    "\n",
    "# Plotting the fully normalized confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(20, 18), dpi=80)\n",
    "\n",
    "mask_zeroes = True\n",
    "if mask_zeroes:\n",
    "    # Define a mask to hide zero values\n",
    "    mask = filtered_confusion_matrix == 0\n",
    "else:\n",
    "    # make a mask that hides nothing\n",
    "    mask = pd.DataFrame(False, index=filtered_confusion_matrix.index, columns=filtered_confusion_matrix.columns)\n",
    "    \n",
    "\n",
    "# Plot the heatmap with zero values masked\n",
    "ax = sns.heatmap(filtered_confusion_matrix, cmap=\"magma_r\",\n",
    "                  cbar=False, yticklabels=True, square=True, linewidths=1, linecolor='white', mask=mask, annot=False, vmax=390)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "# Annotate with appropriate color based on background\n",
    "for (i, j), val in np.ndenumerate(filtered_confusion_matrix):\n",
    "    if not mask.iloc[i, j]:\n",
    "        text_color = 'white' if val > 300 else 'black'\n",
    "        ax.text(j + 0.5, i + 0.5, f'{val}', ha='center', va='center', color=text_color, fontsize=15)\n",
    "\n",
    "# Highlight the diagonal with a black outline\n",
    "for i in range(filtered_confusion_matrix.shape[1]):\n",
    "    ax.add_patch(plt.Rectangle((i, i), 1, 1, fill=False, edgecolor='black', lw=3))\n",
    "\n",
    "# Adjust the limits of the x and y axes to avoid cutting off the outer edges\n",
    "ax.set_xlim(-0.5, filtered_confusion_matrix.shape[1] - 0.5 + 1)\n",
    "ax.set_ylim(filtered_confusion_matrix.shape[0] - 0.5 + 1, -0.5)\n",
    "\n",
    "# add a red vertical line at the 9th column\n",
    "ax.axvline(x=6, ymin = 0.04, ymax = 0.96, color='red', lw=3)\n",
    "# add a red horizontal line at the 10th row\n",
    "ax.axhline(y=6, xmin = 0.05, xmax = 0.95, color='red', lw=3)\n",
    "\n",
    "ax.add_patch(plt.Rectangle((0, 0), 8, 14, fill=False, edgecolor='black', lw=3))\n",
    "\n",
    "for label in ax.get_yticklabels():\n",
    "    x, y = label.get_position()\n",
    "    label.set_position((x + 0.025, y))\n",
    "ax.tick_params(axis='both', width=0)\n",
    "\n",
    "for label in ax.get_xticklabels():\n",
    "    x, y = label.get_position()\n",
    "    label.set_position((x, y  - 0.025))\n",
    "\n",
    "# Add number of starter cells in each area per column on the bottom of the heatmap\n",
    "starter_counts = starters['area_acronym_ancestor_rank1'].value_counts()\n",
    "for i, area in enumerate(filtered_confusion_matrix.columns):\n",
    "    # if area in starter_counts else put 0\n",
    "    ax.text(i + 0.5, filtered_confusion_matrix.shape[0] + 0.5, f'{starter_counts.get(area, 0)}', ha='center', va='center', color='black', fontsize=15)\n",
    "# add a label saying what the sum is\n",
    "ax.text(filtered_confusion_matrix.shape[1] / 2, filtered_confusion_matrix.shape[0] + 1, 'Total starter cells per area', ha='center', va='center', color='black', fontsize=20)\n",
    "\n",
    "# Add number of non-starter cells in each area per row on the right of the heatmap\n",
    "non_starter_counts = filtered_confusion_matrix.sum(axis=1) #non_starters['area_acronym_ancestor_rank1'].value_counts()\n",
    "for i, area in enumerate(filtered_confusion_matrix.index):\n",
    "    # if area in starter_counts else put 0\n",
    "    ax.text(filtered_confusion_matrix.shape[1] + 0.5, i + 0.5, f'{non_starter_counts.get(area, 0)}', ha='center', va='center', color='black', fontsize=15)\n",
    "# add a label saying what the sum is\n",
    "ax.text(filtered_confusion_matrix.shape[1] + 1, filtered_confusion_matrix.shape[0] / 2, 'Total presynaptic cells per area', ha='center', va='center', color='black', fontsize=20, rotation=270)\n",
    "\n",
    "\n",
    "#plt.title('Fully Normalized Confusion Matrix (Row-wise then Column-wise)')\n",
    "plt.xlabel('Starter cell location', fontsize=20, labelpad=20)\n",
    "plt.ylabel('Presynaptic cell location', fontsize=20, labelpad=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "fig_loc = \"/nemo/lab/znamenskiyp/home/users/becalia/figs/thesis/ch5/matrices/\"\n",
    "fig_name = \"matrix_curated_over_4_presyn_only_25_gene_clustered_unnormalised_starter_area_presyn_area_noshared.pdf\"\n",
    "#plt.savefig(fig_loc + fig_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starters = starters[[\"main_barcode\", \"area_acronym_ancestor_rank1\"]]\n",
    "starters = starters.rename(columns={\"area_acronym_ancestor_rank1\": \"starter_area\"})\n",
    "non_starters = non_starters[[\"main_barcode\", \"area_acronym_ancestor_rank1\"]]\n",
    "non_starters = non_starters.rename(columns={\"area_acronym_ancestor_rank1\": \"presyn_area\"})\n",
    "starter_areas_to_keep = ['VISp5', 'VISp4', 'VISp2/3', 'VISp6a', 'VISp1', 'VISp6b']\n",
    "starters = starters[starters['starter_area'].isin(starter_areas_to_keep)]\n",
    "starters_barcodes = starters.main_barcode.unique()\n",
    "non_starters = non_starters[non_starters[\"main_barcode\"].isin(starters_barcodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute shuffled permutations of connectivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_observed_confusion_matrix(starters, non_starters):\n",
    "    \"\"\"\n",
    "    Vectorized computation of the *observed* confusion matrix.\n",
    "    \n",
    "    We merge on 'main_barcode' (starters <-> non_starters),\n",
    "    group by (presyn_area, starter_area) to count connections,\n",
    "    and pivot into a DataFrame.\n",
    "    \"\"\"\n",
    "    merged = pd.merge(\n",
    "        non_starters[['presyn_area', 'main_barcode']],\n",
    "        starters[['starter_area', 'main_barcode']],\n",
    "        on='main_barcode',\n",
    "        how='inner'\n",
    "    )\n",
    "    grouped = merged.groupby(['presyn_area', 'starter_area']).size().reset_index(name='count')\n",
    "    confusion_df = grouped.pivot_table(index='presyn_area', columns='starter_area', values='count', fill_value=0)\n",
    "    return confusion_df\n",
    "\n",
    "\n",
    "def shuffle_cm_chunk(non_starters_arr, starters_arr, row_index, col_index, n_permutations, seed_offset=0):\n",
    "    \"\"\"\n",
    "    Perform *n_permutations* random shuffles in one process.\n",
    "    Return a list of confusion-matrix arrays, each reindexed\n",
    "    to match (row_index, col_index).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    non_starters_arr : dict with keys {'presyn_area', 'barcodes'} \n",
    "    starters_arr : dict with keys {'starter_area', 'barcodes'}\n",
    "    row_index : pd.Index for the final matrix rows\n",
    "    col_index : pd.Index for the final matrix columns\n",
    "    n_permutations : int\n",
    "    seed_offset : int, optional random seed offset\n",
    "    \"\"\"\n",
    "    np.random.seed(seed_offset)\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(n_permutations):\n",
    "        # 1) Shuffle\n",
    "        shuffled_barcodes = np.random.permutation(non_starters_arr['barcodes'])\n",
    "        \n",
    "        # 2) Build a merged DataFrame with vectorized approach\n",
    "        ns_df = pd.DataFrame({\n",
    "            'presyn_area': non_starters_arr['presyn_area'],\n",
    "            'main_barcode': shuffled_barcodes\n",
    "        })\n",
    "        st_df = pd.DataFrame({\n",
    "            'starter_area': starters_arr['starter_area'],\n",
    "            'main_barcode': starters_arr['barcodes']\n",
    "        })\n",
    "        \n",
    "        merged = pd.merge(ns_df, st_df, on='main_barcode', how='inner')\n",
    "        \n",
    "        # 3) groupby -> pivot\n",
    "        grouped = merged.groupby(['presyn_area', 'starter_area']).size().reset_index(name='count')\n",
    "        shuffle_cm = grouped.pivot_table(index='presyn_area', columns='starter_area', values='count', fill_value=0)\n",
    "        \n",
    "        # 4) Reindex to match the final shape (row_index, col_index)\n",
    "        shuffle_cm = shuffle_cm.reindex(index=row_index, columns=col_index, fill_value=0)\n",
    "        \n",
    "        # 5) Append the numpy array version\n",
    "        results.append(shuffle_cm.values)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main_parallel_shuffling(starters, non_starters, n_permutations=10000, n_jobs=8):\n",
    "    \"\"\"\n",
    "    Orchestrates the parallel shuffling procedure:\n",
    "    1) Compute observed confusion matrix (vectorized).\n",
    "    2) Prepare minimal data structures for pickling.\n",
    "    3) Chunk the total permutations into fewer tasks.\n",
    "    4) Parallelize with ProcessPoolExecutor.\n",
    "    5) Return list of all null matrices (arrays).\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute observed confusion matrix in a vectorized way\n",
    "    observed_confusion_matrix = compute_observed_confusion_matrix(starters, non_starters)\n",
    "    \n",
    "    # Save row/col indices for consistent reindexing\n",
    "    row_index = observed_confusion_matrix.index\n",
    "    col_index = observed_confusion_matrix.columns\n",
    "    \n",
    "    # Prepare minimal data for pickling\n",
    "    non_starters_arr = {\n",
    "        'presyn_area': non_starters['presyn_area'].values,\n",
    "        'barcodes': non_starters['main_barcode'].values\n",
    "    }\n",
    "    starters_arr = {\n",
    "        'starter_area': starters['starter_area'].values,\n",
    "        'barcodes': starters['main_barcode'].values\n",
    "    }\n",
    "    \n",
    "    # Chunk permutations into fewer tasks\n",
    "    all_null_matrices = []\n",
    "    chunk_size = max(n_permutations // n_jobs, 1)\n",
    "    remainder  = n_permutations % n_jobs\n",
    "    tasks = []\n",
    "    for i in range(n_jobs):\n",
    "        this_chunk = chunk_size + (1 if i < remainder else 0)\n",
    "        tasks.append(this_chunk)\n",
    "    \n",
    "\n",
    "    # Parallel loop\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        futures = []\n",
    "        seed_offset = 42\n",
    "        for chunk in tasks:\n",
    "            if chunk > 0:\n",
    "                future = executor.submit(\n",
    "                    shuffle_cm_chunk,\n",
    "                    non_starters_arr,\n",
    "                    starters_arr,\n",
    "                    row_index,\n",
    "                    col_index,\n",
    "                    chunk,\n",
    "                    seed_offset\n",
    "                )\n",
    "                futures.append(future)\n",
    "                seed_offset += 1\n",
    "        \n",
    "        # Wrap as_completed in tqdm for progress\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Shuffling\"):\n",
    "            chunk_result = f.result()\n",
    "            all_null_matrices.extend(chunk_result)\n",
    "    print(f\"Done! Generated {len(all_null_matrices)} shuffled matrices.\")\n",
    "    return observed_confusion_matrix, all_null_matrices\n",
    "\n",
    "\n",
    "\n",
    "# Run the parallel shuffling\n",
    "n_permutations = 100000\n",
    "n_jobs = 250\n",
    "observed_cm, all_nulls = main_parallel_shuffling(starters, non_starters, \n",
    "                                                    n_permutations=n_permutations, \n",
    "                                                    n_jobs=n_jobs)\n",
    "\n",
    "print(\"Observed Confusion Matrix:\")\n",
    "print(observed_cm)\n",
    "print(f\"Example shuffled matrix shape: {all_nulls[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot null distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_null_histograms_square(\n",
    "    observed_cm, \n",
    "    all_null_matrices, \n",
    "    bins=20, \n",
    "    row_label_fontsize=14, \n",
    "    col_label_fontsize=14,\n",
    "    row_order=None,\n",
    "    col_order=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a grid of square histogram subplots, one for each cell in the observed confusion matrix,\n",
    "    with optional custom ordering/filtering of rows (presynaptic areas) and columns (starter areas).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observed_cm : pd.DataFrame\n",
    "        Observed confusion matrix (rows = presyn areas, columns = starter areas).\n",
    "    all_null_matrices : list of np.ndarray\n",
    "        List of length N_permutations, each a 2D array (n_rows x n_cols) from a shuffle,\n",
    "        with the same row/col alignment as observed_cm.\n",
    "    bins : int\n",
    "        Number of histogram bins.\n",
    "    row_label_fontsize : int\n",
    "        Font size for the row (presyn area) label on the left edge.\n",
    "    col_label_fontsize : int\n",
    "        Font size for the column (starter area) label on the top edge.\n",
    "    row_order : list or None\n",
    "        List of row labels (presynaptic areas) to include and in which order.\n",
    "        If None, use observed_cm.index as-is.\n",
    "    col_order : list or None\n",
    "        List of column labels (starter areas) to include and in which order.\n",
    "        If None, use observed_cm.columns as-is.\n",
    "    \"\"\"\n",
    "\n",
    "    if row_order is None:\n",
    "        row_order = list(observed_cm.index)\n",
    "    if col_order is None:\n",
    "        col_order = list(observed_cm.columns)\n",
    "    row_order = [r for r in row_order if r in observed_cm.index]\n",
    "    col_order = [c for c in col_order if c in observed_cm.columns]\n",
    "\n",
    "    # Subset the cms\n",
    "    subset_observed_cm = observed_cm.loc[row_order, col_order]\n",
    "    n_rows, n_cols = subset_observed_cm.shape\n",
    "    null_array = np.array(all_null_matrices)\n",
    "    row_indices = [observed_cm.index.get_loc(r) for r in row_order]\n",
    "    col_indices = [observed_cm.columns.get_loc(c) for c in col_order]\n",
    "    subset_null_array = null_array[:, row_indices][:, :, col_indices]\n",
    "\n",
    "    # Create the figure and axes grid\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "        figsize=(n_cols * 2.0, n_rows * 2.0),\n",
    "        sharex=False,\n",
    "        sharey=False\n",
    "    )\n",
    "\n",
    "    def get_ax(i, j):\n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            return axes\n",
    "        elif n_rows == 1:\n",
    "            return axes[j]\n",
    "        elif n_cols == 1:\n",
    "            return axes[i]\n",
    "        else:\n",
    "            return axes[i, j]\n",
    "    \n",
    "    # Plot each histogram\n",
    "    for i, row_label in enumerate(row_order):\n",
    "        for j, col_label in enumerate(col_order):\n",
    "            ax = get_ax(i, j)\n",
    "            cell_values = subset_null_array[:, i, j]\n",
    "            observed_val = subset_observed_cm.iloc[i, j]\n",
    "            ax.hist(cell_values, bins=bins, density=True, alpha=0.5, color='black')\n",
    "            ax.axvline(observed_val, color='red', linewidth=2)\n",
    "        \n",
    "            ax.set_yticks([])\n",
    "            ax.set_yticklabels([])\n",
    "            if j == 0:\n",
    "                ax.text(\n",
    "                    -0.3, 0.5, \n",
    "                    str(row_label), \n",
    "                    rotation=90, \n",
    "                    ha='center', va='center',\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=row_label_fontsize\n",
    "                )\n",
    "            if i == 0:\n",
    "                ax.text(\n",
    "                    0.5, 1.2, \n",
    "                    str(col_label), \n",
    "                    ha='center', va='bottom',\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=col_label_fontsize\n",
    "                )\n",
    "                \n",
    "    plt.suptitle(\"Starter cell area\", fontsize=16, y=0.93)\n",
    "    plt.text(0.06, 0.5, \"Presynaptic cell area\", fontsize=16, rotation=90, \n",
    "             ha='center', va='center', transform=fig.transFigure)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return subset_null_array, subset_observed_cm\n",
    "\n",
    "subset_null_array, subset_observed_cm = plot_null_histograms_square(observed_cm, all_nulls, bins=30, row_order=['VISp1', 'VISp2/3', 'VISp4', 'VISp5', 'VISp6a', 'VISp6b', 'VISal','VISl', 'VISli', 'VISpm', 'RSP', 'AUD', 'TEa', 'TH'], col_order = [\"VISp1\",\"VISp2/3\",\"VISp4\",\"VISp5\",\"VISp6a\",\"VISp6b\",\"VISl\",\"VISpm\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute log ratio and z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_null = subset_null_array.mean(axis=0)\n",
    "std_null  = subset_null_array.std(axis=0)\n",
    "z_matrix = (subset_observed_cm.values - mean_null) / (std_null + 1e-9)\n",
    "z_matrix = pd.DataFrame(z_matrix, index=subset_observed_cm.index, columns=subset_observed_cm.columns)\n",
    "\n",
    "# Calculate the ratio matrix\n",
    "ratio_matrix = subset_observed_cm.values / (mean_null + 1e-9)\n",
    "ratio_matrix = pd.DataFrame(ratio_matrix, index=subset_observed_cm.index, columns=subset_observed_cm.columns)\n",
    "# Calculate the log ratio matrix\n",
    "log_ratio_matrix = np.log10(ratio_matrix)\n",
    "mask = np.isclose(log_ratio_matrix, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    log_ratio_matrix, \n",
    "    cmap='coolwarm',    # negative (blue) to positive (red)\n",
    "    center=0,           # center at zero for a balanced colormap\n",
    "    annot=True,         # show the actual log ratio numbers\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    mask=mask,          # mask the zero values\n",
    "    cbar_kws={'label': 'Log10 Ratio'},  # label the colorbar\n",
    "    vmax=0.75,             # adjust as needed for your data\n",
    "    vmin=-0.75             # adjust as needed for your data\n",
    ")\n",
    "plt.title(\"Log Ratio of Observed vs. Shuffled Null\")\n",
    "plt.xlabel(\"Starter area\")\n",
    "plt.ylabel(\"Presynaptic area\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    z_matrix, \n",
    "    cmap='coolwarm',    # negative (blue) to positive (red)\n",
    "    center=0,           # center at zero for a balanced colormap\n",
    "    annot=True,         # show the actual z-score numbers\n",
    "    fmt=\".1f\",\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Z-score'},  # label the colorbar\n",
    "    vmax=4\n",
    ")\n",
    "plt.title(\"Z-score of Observed vs. Shuffled Null\")\n",
    "plt.xlabel(\"Starter area\")\n",
    "plt.ylabel(\"Presynaptic area\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute empirical p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_empirical_pvalues(\n",
    "    observed_cm, \n",
    "    null_array, \n",
    "    two_sided=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute empirical p-values for each cell in the observed confusion matrix\n",
    "    based on the distribution of values in all_null_matrices, using a small-sample correction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observed_cm : pd.DataFrame\n",
    "        The observed confusion matrix (n_rows x n_cols).\n",
    "    all_null_matrices : 3D np.ndarray\n",
    "        An array of N_permutations, each a 2D array with shape (n_rows, n_cols),\n",
    "        representing the shuffled null distributions. Must align with observed_cm rows/cols.\n",
    "    two_sided : bool\n",
    "        If True, compute two-sided empirical p-values. Otherwise, compute right-tailed p-values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pval_df : pd.DataFrame\n",
    "        A DataFrame (same shape as observed_cm) with the empirical p-values.\n",
    "    \"\"\"\n",
    "    n_permutations = null_array.shape[0]\n",
    "    n_rows, n_cols = observed_cm.shape\n",
    "\n",
    "    # Prepare an output DataFrame for p-values\n",
    "    pval_df = pd.DataFrame(\n",
    "        np.zeros((n_rows, n_cols)), \n",
    "        index=observed_cm.index, \n",
    "        columns=observed_cm.columns, \n",
    "        dtype=float\n",
    "    )\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            observed_val = observed_cm.iat[i, j]\n",
    "            \n",
    "            # If there's no observed value (NaN), set p-value to NaN and continue\n",
    "            if pd.isna(observed_val):\n",
    "                pval_df.iat[i, j] = np.nan\n",
    "                continue\n",
    "            \n",
    "            # Extract the null distribution for this cell across permutations\n",
    "            cell_null_vals = null_array[:, i, j]\n",
    "            \n",
    "            # Count how many are >= and <= the observed\n",
    "            count_ge = np.sum(cell_null_vals >= observed_val)\n",
    "            count_le = np.sum(cell_null_vals <= observed_val)\n",
    "            \n",
    "            # Small-sample correction uses (count + 1)/(N + 1)\n",
    "            p_right = (count_ge + 1) / (n_permutations + 1)\n",
    "            p_left  = (count_le + 1) / (n_permutations + 1)\n",
    "            \n",
    "            if two_sided:\n",
    "                # Two-sided p-value\n",
    "                p_2sided = 2.0 * min(p_left, p_right)\n",
    "                # Clamp at 1\n",
    "                p_val = min(p_2sided, 1.0)\n",
    "            else:\n",
    "                # One-sided (right-tailed)\n",
    "                p_val = p_right\n",
    "            \n",
    "            pval_df.iat[i, j] = p_val\n",
    "\n",
    "    return pval_df\n",
    "   \n",
    "\n",
    "pval_df = compute_empirical_pvalues(subset_observed_cm, subset_null_array, two_sided=True)\n",
    "\n",
    "def custom_format(x):\n",
    "    if x == 0:\n",
    "        return \"0\"  # Avoid scientific notation for zeros\n",
    "    formatted = \"{:.1e}\".format(x)  # Format as scientific notation with 1 dp\n",
    "    base, exp = formatted.split(\"e\")  # Split into base and exponent\n",
    "    return f\"{base}e{int(exp)}\"  # Remove leading zeros from exponent\n",
    "\n",
    "# Convert dataframe to formatted strings\n",
    "formatted_pval_df = pval_df.applymap(custom_format)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    pval_df, \n",
    "    cmap='Blues',    \n",
    "    center=0,        \n",
    "    annot=formatted_pval_df,\n",
    "    fmt=\"\",\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'P-value'},  \n",
    "    vmax=0.05\n",
    ")\n",
    "plt.title(\"P-value of Observed vs. Shuffled Null\")\n",
    "plt.xlabel(\"Starter area\")\n",
    "plt.ylabel(\"Presynaptic area\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure pval_df has the same index/columns as log_ratio_matrix\n",
    "pval_df = pval_df.loc[log_ratio_matrix.index, log_ratio_matrix.columns]\n",
    "\n",
    "# Function to format p-values in scientific notation (1 decimal before 'e')\n",
    "def format_pval(x):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    formatted = \"{:.1e}\".format(x)  # Format with 1 decimal\n",
    "    base, exp = formatted.split(\"e\")  # Split into base and exponent\n",
    "    return f\"{base}e{int(exp)}\"  # Remove leading zeros in exponent\n",
    "\n",
    "# Identify significant cells where p-value < 0.05\n",
    "significant_cells = pval_df < 0.05\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(\n",
    "    log_ratio_matrix, \n",
    "    cmap='coolwarm',    \n",
    "    center=0,           \n",
    "    annot=False,\n",
    "    square=True,\n",
    "    mask=mask,          \n",
    "    cbar_kws={'label': 'Log10 Ratio'},  \n",
    "    vmax=0.30,          \n",
    "    vmin=-0.30          \n",
    ")\n",
    "\n",
    "# Manually add text annotations\n",
    "for i in range(log_ratio_matrix.shape[0]):\n",
    "    for j in range(log_ratio_matrix.shape[1]):\n",
    "        log_ratio = log_ratio_matrix.iloc[i, j]\n",
    "        p_val = pval_df.iloc[i, j]\n",
    "        p_val_text = format_pval(p_val)\n",
    "\n",
    "        # If log_ratio is finite (not -inf), display it\n",
    "        if np.isfinite(log_ratio):\n",
    "            ax.text(j + 0.5, i + 0.4, f\"{log_ratio:.2f}\", ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "        # Always display the p-value in smaller text\n",
    "        ax.text(j + 0.5, i + 0.7, p_val_text, ha='center', va='center', fontsize=8, color='black')\n",
    "\n",
    "        # Add black outline if significant (p < 0.05)\n",
    "        if significant_cells.iloc[i, j]:\n",
    "            rect = plt.Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=2)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "\n",
    "ax.axhline(y=0, color='grey', linewidth=5)\n",
    "ax.axhline(y=log_ratio_matrix.shape[0], color='grey', linewidth=5)\n",
    "ax.axvline(x=0, color='grey', linewidth=5)\n",
    "ax.axvline(x=log_ratio_matrix.shape[1], color='grey', linewidth=5)\n",
    "ax.set_xlim(0, log_ratio_matrix.shape[1])\n",
    "ax.set_ylim(log_ratio_matrix.shape[0], 0)\n",
    "\n",
    "plt.title(\"Log Ratio of Observed vs. Shuffled Null with P-values\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Starter area\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Presynaptic area\", fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot bubble plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_plot(\n",
    "    log_ratio_matrix, \n",
    "    pval_df, \n",
    "    size_scale=800\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a bubble plot to visualize log-ratios with p-values.\n",
    "    Args:\n",
    "    - log_ratio_matrix: pd.DataFrame, shape (n_rows, n_cols)\n",
    "        DataFrame of log-ratios (log₁₀(observed / expected)).\n",
    "    - pval_df: pd.DataFrame, shape (n_rows, n_cols)\n",
    "        DataFrame of p-values for each log-ratio.\n",
    "    - size_scale: int, default 800\n",
    "        Scaling factor for bubble sizes.\n",
    "    \"\"\"\n",
    "    # Reformat input dfs into a long-form DataFrame for plotting\n",
    "    row_name = log_ratio_matrix.index.name   or \"row_label\"\n",
    "    col_name = log_ratio_matrix.columns.name or \"col_label\"\n",
    "    df_plot = (\n",
    "        log_ratio_matrix\n",
    "        .stack() \n",
    "        .reset_index()\n",
    "        .rename(columns={\n",
    "            row_name: \"y_label\",\n",
    "            col_name: \"x_label\",\n",
    "            0:        \"log_ratio\"\n",
    "        })\n",
    "    )\n",
    "    df_plot[\"p_value\"] = pval_df.stack().values\n",
    "    df_plot[\"x\"] = pd.Categorical(\n",
    "        df_plot[\"x_label\"], \n",
    "        categories=log_ratio_matrix.columns,\n",
    "        ordered=True\n",
    "    ).codes\n",
    "    df_plot[\"y\"] = pd.Categorical(\n",
    "        df_plot[\"y_label\"], \n",
    "        categories=log_ratio_matrix.index,\n",
    "        ordered=True\n",
    "    ).codes\n",
    "    x_categories = log_ratio_matrix.columns\n",
    "    y_categories = log_ratio_matrix.index\n",
    "\n",
    "\n",
    "    # Calculate bubble size & color value\n",
    "    # Bubble size: absolute log ratio * size_scale\n",
    "    df_plot[\"bubble_size\"] = df_plot[\"log_ratio\"].abs() * size_scale\n",
    "    # Color value = sign(log_ratio) * -log10(p_value)\n",
    "    # => Positive log-ratio => red, negative => blue\n",
    "    df_plot[\"color_value\"] = (\n",
    "        np.sign(df_plot[\"log_ratio\"]) *\n",
    "        -np.log10(df_plot[\"p_value\"].clip(lower=1e-300))\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "\n",
    "    # Main scatter\n",
    "    sc = ax.scatter(\n",
    "        x     = df_plot[\"x\"],\n",
    "        y     = df_plot[\"y\"],\n",
    "        s     = df_plot[\"bubble_size\"],\n",
    "        c     = df_plot[\"color_value\"],\n",
    "        cmap  = \"coolwarm\",\n",
    "        vmin  = df_plot[\"color_value\"].min(),\n",
    "        vmax  = df_plot[\"color_value\"].max(),\n",
    "        edgecolors=\"none\"\n",
    "    )\n",
    "\n",
    "    # Add black outlines for significant cells\n",
    "    significant_cells = (pval_df < 0.05)\n",
    "    is_signif = significant_cells.stack().values\n",
    "    df_signif = df_plot[is_signif]\n",
    "    ax.scatter(\n",
    "        x          = df_signif[\"x\"],\n",
    "        y          = df_signif[\"y\"],\n",
    "        s          = df_signif[\"bubble_size\"],\n",
    "        facecolors = \"none\",\n",
    "        edgecolors = \"black\",\n",
    "        linewidths = 1.2\n",
    "    )\n",
    "\n",
    "    cbar = plt.colorbar(sc, ax=ax)\n",
    "    cbar.set_label(\"Sign(log₁₀(ratio)) × -log₁₀(p-value)\", fontsize=12)\n",
    "\n",
    "    # Bubble-size legend\n",
    "    legend_values = [0.1, 0.3, 0.6]\n",
    "    legend_handles = []\n",
    "    for val in legend_values:\n",
    "        size_val = val * size_scale\n",
    "        h = ax.scatter([], [], s=size_val, c=\"gray\", alpha=0.5,\n",
    "                       label=f\"|log₁₀(ratio)| = {val}\")\n",
    "        legend_handles.append(h)\n",
    "    ax.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Bubble Size Legend\",\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.05, -0.05),\n",
    "        borderaxespad=0.,\n",
    "        frameon=True,\n",
    "        handleheight=4.0\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(range(len(x_categories)))\n",
    "    ax.set_yticks(range(len(y_categories)))\n",
    "    ax.set_xticklabels(x_categories, rotation=90)\n",
    "    ax.set_yticklabels(y_categories)\n",
    "    # Invert y-axis so top row is y=0\n",
    "    ax.invert_yaxis()\n",
    "    plt.xlabel(\"Starter area\", fontsize=13, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Presynaptic area\", fontsize=13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "bubble_plot(log_ratio_matrix, pval_df, size_scale=1500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iss-analysis-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
